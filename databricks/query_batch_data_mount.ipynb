{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f024d03f-ee06-4059-ae33-defee197a946",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Batch Data Processing using Spark on Databricks\n",
    "\n",
    "Apache Spark is a powerful open-source distributed computing system that provides fast and general-purpose cluster computing for big data processing.\n",
    "\n",
    "Databricks, on the other hand, is a cloud-based platform built on top of Apache Spark, making it easier to deploy and manage Spark clusters. \n",
    "Databricks provides a unified analytics platform that can process large amounts of data quickly. \n",
    "Databricks provides an optimised and managed Spark environment.\n",
    "\n",
    "To clean and query the data from the three Kafka topics, the S3 bucket will be mounted to a Databricks account. Within Databricks three DataFrames will be created to hold this data:\n",
    "\n",
    "- `df_pin` for the Pinterest post data\n",
    "- `df_geo` for the geolocation data\n",
    "- `df_user` for the user data.\n",
    "\n",
    "This notebook will falcitate the following procedures:\n",
    "\n",
    "- Load the batch data\n",
    "- Clean the batch data\n",
    "- Query the batch data\n",
    "\n",
    "This notebook uses the ***mounting*** methods and dataframe creation methods from the `databricks_load_data` notebook located in the `classes` folder.\n",
    "\n",
    "This notebook also uses the dataframe cleaning methods from the `databricks_clean_data` file also located in the `classes` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a45857-0aa5-4f67-a004-1e6a8272df00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Import loading methods \n",
    "\n",
    "The following cell allows access to the methods from the `S3DataLoader` class within the `databricks_load_data` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0892dad5-07be-47af-92f0-8c78f2a9ec6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./classes/databricks_load_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749d85ea-8678-4b2c-be23-01161c71ff3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Import cleaning methods\n",
    "\n",
    "The following cell allows access to the methods from the `DataCleaning` class within the `databricks_clean_data` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81728018-855c-41bd-8bd2-bb3066176583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./classes/databricks_clean_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fce496d-dfd7-4f18-8944-b492a3d9f541",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Instantiate S3DataLoader and DataCleaning\n",
    "\n",
    "The following cell instantiates the required variables for the `S3DataLoader` class and `DataCleaning` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19ae7dc-e4f6-48da-a193-d9ff9ae2c2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    credentials_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "    iam_username = \"0ab336d6fcf7\"\n",
    "    topics = ['pin', 'geo', 'user']\n",
    "    data_loader = S3DataLoader(credentials_path, iam_username, topics)\n",
    "    data_cleaner = DataCleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd7bc0e-76a1-4991-aa1f-74307a566824",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Batch Data\n",
    "\n",
    "Databricks enables users to mount cloud object storage to the Databricks File System (DBFS) to simplify data access patterns.\n",
    "\n",
    "Databricks mounts create a link between a workspace and cloud object storage, which enables interaction with cloud object storage using familiar file paths relative to the Databricks file system. Mounts work by creating a local alias under the /mnt directory that stores the following information:\n",
    "\n",
    "- Location of the cloud object storage.\n",
    "- Driver specifications to connect to the storage account or container.\n",
    "- Security credentials required to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2922bab8-1c1c-4ad9-9d0a-685779ff3000",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Mount S3 bucket\n",
    "\n",
    "As per the project instructions the following cell mounts an external S3 bucket to Databricks Filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9d7e2c-fa42-465a-8fac-2d02c2b6d562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load AWS keys and configure S3 bucket\n",
    "    data_loader.mount_s3_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff10143-df97-4ad9-9379-1c2f1386eb36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Show file structure\n",
    "\n",
    "The following cell will show the file structure within the mount path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1eec23-4d91-43e2-bd82-c35148248a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Display the mounted S3 bucket\n",
    "    data_loader.display_s3_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8693cfc-3cbb-4c34-bc1f-a648b849778a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create dataframes\n",
    "\n",
    "The following cell will create three dataframes from the data stored in the mount path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6fc76a-4698-483e-a415-e884cc93aacb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create dataframes\n",
    "    data_loader.create_dataframes(mounted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25f9e0f-c8be-4def-b80d-02399e2a89a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clean Batch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6783367-fb60-469b-b676-48df6a70850a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Clean df.pin\n",
    "\n",
    "To clean the `df_pin` DataFrame the following cell will perform the following transformations:\n",
    "\n",
    "- Replace empty entries and entries with no relevant data in each column with `Nones`\n",
    "- Perform the necessary transformations on the `follower_count` to ensure every entry is a number. Make sure the data type of this column is an `int`.\n",
    "- Ensure that each column containing numeric data has a `numeric` data type\n",
    "- Clean the data in the `save_location` column to include only the save location path\n",
    "- Rename the `index` column to `ind`.\n",
    "- Reorder the DataFrame columns to have the following column order: (`ind`, `unique_id`, `title`, `description`, `follower_count`, `poster_name`, `tag_list`, `is_image_or_video`, `image_src`, `save_location`, `category`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32d94a6-fa5d-43b6-a076-8735ab0bd2bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cleaned_df_pin = data_cleaner.clean_pin_data(df_pin)\n",
    "\n",
    "    print(\"Schema for original dataframe\")\n",
    "    df_pin.printSchema()\n",
    "    print(\"Schema for cleaned dataframe\")\n",
    "    cleaned_df_pin.printSchema()\n",
    "\n",
    "    print(\"Original dataframe:\")\n",
    "    display(df_pin)\n",
    "    print(\"Cleaned dataframe:\")\n",
    "    display(cleaned_df_pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c909d3-db55-4c64-93ab-2e886521a384",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Clean df.geo\n",
    "\n",
    "To clean the `df_geo` DataFrame the follwoing cell will perform the following transformations:\n",
    "\n",
    "- Create a new column `coordinates` that contains an array based on the `latitude` and `longitude` columns\n",
    "- Drop the `latitude` and `longitude` columns from the DataFrame\n",
    "- Convert the `timestamp` column from a `string` to a `timestamp` data type\n",
    "- Reorder the DataFrame columns to have the following column order: (`ind`, `country`, `coordinates`, `timestamp`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e59ec2-9e6a-412e-a388-69646e5e4990",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cleaned_df_geo = data_cleaner.clean_geo_data(df_geo)\n",
    "\n",
    "    print(\"Schema for original dataframe\")\n",
    "    df_geo.printSchema()\n",
    "    print(\"Schema for cleaned dataframe\")\n",
    "    cleaned_df_geo.printSchema()\n",
    "\n",
    "    print(\"Original dataframe:\")\n",
    "    display(df_geo)\n",
    "    print(\"Cleaned dataframe:\")\n",
    "    display(cleaned_df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fcd0cc-e1f2-4af5-b5c9-2ee17b284edf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Clean df.user\n",
    "\n",
    "To clean the `df_user` DataFrame the following cell will perform the following transformations:\n",
    "\n",
    "- Create a new column user_name that concatenates the information found in the `first_name` and `last_name` columns\n",
    "- Drop the `first_name` and `last_name` columns from the DataFrame\n",
    "- Convert the `date_joined` column from a `string` to a `timestamp` data type\n",
    "- Reorder the DataFrame columns to have the following column order: (`ind`, `user_name`, `age`, `date_joined`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f529ac18-e09c-45f5-bf6c-f572672e8840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cleaned_df_user = data_cleaner.clean_user_data(df_user)\n",
    "\n",
    "    print(\"Schema for original dataframe\")\n",
    "    df_user.printSchema()\n",
    "    print(\"Schema for cleaned dataframe\")\n",
    "    cleaned_df_user.printSchema()\n",
    "    \n",
    "    print(\"Original dataframe:\")\n",
    "    display(df_user)\n",
    "    print(\"Cleaned dataframe:\")\n",
    "    display(cleaned_df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1e8968-b719-4193-8e9a-e6d0b116e4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Query Batch Data\n",
    "\n",
    "Before querieing the data the three dataframes (`df_pin`, `df_geo`, and `df_user`) are joined together on the common column heading `ind`.\n",
    "\n",
    "To make sure that `df_all` is a valid DataFrame it will be created and registered as a temporary table or view before executing any SQL queries. To do this `df_all` is registered as a temporary view using `df_all.createOrReplaceTempView(\"df_all\")`.\n",
    "\n",
    "However `df_all` is a non-Delta table with many small files. Therefore to improve the performance of queries, `df_all` has been converted to Delta. The new `df_all` table will accelerate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd17239f-a2d7-4c30-ad5c-4e53415ff350",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Join the three dataframes\n",
    "    df_all = cleaned_df_pin.join(cleaned_df_geo, 'ind').join(cleaned_df_user, 'ind')\n",
    "\n",
    "    # Write the Delta table\n",
    "    delta_table_path = \"/delta/my_table/mount\"\n",
    "    df_all.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "    # Use DeltaTable API to optimize the Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "    delta_table.vacuum()\n",
    "    df_all = delta_table.toDF()\n",
    "\n",
    "    # Create a temperory table of the combined dataframes\n",
    "    df_all.createOrReplaceTempView(\"df_all\")\n",
    "\n",
    "    # Display the optimized Delta tables\n",
    "    display(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dddcc536-7383-46bd-abeb-6ca511977ef8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 1: Find the most popular category in each country\n",
    "\n",
    "- Find the most popular Pinterest category people post to based on their country.\n",
    "- The query should return a DataFrame that contains the following columns: (`country`, `category`, `category_count`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79e9ba5-1b99-4231-8c8a-f6f85f2cfad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_category_per_country = spark.sql(\"\"\"\n",
    "    WITH ranked_categories AS (\n",
    "        SELECT\n",
    "            country, \n",
    "            category, \n",
    "            COUNT(category) AS category_count, \n",
    "            RANK() OVER (PARTITION BY country ORDER BY count(category) DESC) AS category_rank \n",
    "        FROM df_all \n",
    "        GROUP BY country, category \n",
    "    ) \n",
    "    SELECT country, category, category_count \n",
    "    FROM ranked_categories \n",
    "    WHERE category_rank = 1 \n",
    "    ORDER BY country\n",
    "\"\"\")\n",
    "display(top_category_per_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25719b57-44e8-4a94-9eb2-7528666c98c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 2: Find which was the most popular category each year\n",
    "\n",
    "- Find how many posts each category had between 2018 and 2022.\n",
    "- The query will return a DataFrame that contains the following columns: (`post_year`, `category`, `category_count`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece600b5-7bb9-490f-ada2-77a141534f2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_category_per_year = spark.sql(\"\"\"\n",
    "    WITH ranked_categories AS (\n",
    "        SELECT\n",
    "            category,\n",
    "            EXTRACT(YEAR FROM timestamp) AS post_year,\n",
    "            COUNT(category) AS category_count,\n",
    "            RANK() OVER (PARTITION BY EXTRACT(YEAR FROM timestamp) ORDER BY count(category) DESC) AS category_rank \n",
    "        FROM df_all \n",
    "        GROUP BY EXTRACT(YEAR FROM timestamp), category\n",
    "    )\n",
    "    SELECT post_year, category, category_count \n",
    "    FROM ranked_categories \n",
    "    WHERE category_rank = 1 AND post_year BETWEEN 2018 AND 2022\n",
    "    ORDER BY post_year\n",
    "\"\"\")\n",
    "display(top_category_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9424990-0046-4f9b-b4ff-c73f3197a421",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 3: Find the user with most followers in each country\n",
    "\n",
    "- Step 1: For each country find the user with the most followers.\n",
    "  - Your query should return a DataFrame that contains the following columns: (`country`, `poster_name`, `follower_count`)\n",
    "- Step 2: Based on the above query, find the country with the user with most followers.\n",
    "  - Your query should return a DataFrame that contains the following columns: (`country`, `follower_count`)\n",
    "  - This DataFrame should have only one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cff21e0-fc95-44a8-81df-fd4757fb7826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "top_user_per_country = spark.sql(\"\"\"\n",
    "    SELECT country, poster_name, MAX(follower_count) AS follower_count\n",
    "    FROM df_all\n",
    "    GROUP BY country, poster_name\n",
    "    ORDER BY follower_count DESC\n",
    "\"\"\")\n",
    "display(top_user_per_country)\n",
    "\n",
    "# Step 2:\n",
    "country_with_top_user = spark.sql(\"\"\"\n",
    "    WITH top_users AS (\n",
    "        SELECT country, poster_name, MAX(follower_count) AS follower_count\n",
    "        FROM df_all\n",
    "        GROUP BY country, poster_name\n",
    "        ORDER BY follower_count DESC\n",
    "    )\n",
    "    SELECT country, follower_count\n",
    "    FROM top_users\n",
    "    ORDER BY follower_count DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "display(country_with_top_user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f13ba6-907d-4ca9-a726-2433a4cb2633",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 4: Find the most popular category for different age groups\n",
    "\n",
    "- What is the most popular category people post to based on the following age groups: (`18-24`, `25-35`, `36-50`, `+50`)\n",
    "- The query should return a DataFrame that contains the following columns: (`age_group`, `category`, `category_count`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7981692-4fe6-4ba6-90d2-1ec131ad1ecb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_category_per_age = spark.sql(\"\"\"\n",
    "    WITH age_groups AS (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN Age BETWEEN 18 AND 24 THEN '18-24'\n",
    "                WHEN Age BETWEEN 25 AND 35 THEN '25-35'\n",
    "                WHEN Age BETWEEN 36 AND 50 THEN '36-50'\n",
    "                WHEN Age >= 51 THEN '50+'\n",
    "                ELSE 'NA'\n",
    "            END AS age_group,\n",
    "            category, \n",
    "            COUNT(category) AS category_count\n",
    "        FROM df_all\n",
    "        GROUP BY age_group, category\n",
    "    ),\n",
    "    ranked_ages AS (\n",
    "        SELECT \n",
    "            age_group, \n",
    "            category, \n",
    "            category_count, \n",
    "            RANK() OVER (PARTITION BY age_group ORDER BY category_count DESC) AS category_rank\n",
    "        FROM age_groups\n",
    "    )\n",
    "    SELECT age_group, category, category_count\n",
    "    FROM ranked_ages\n",
    "    WHERE category_rank = 1\n",
    "\"\"\")\n",
    "display(top_category_per_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16a9c93-a403-4873-a5d0-5b69dc2e3a97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 5: Find the median follower count for different age groups\n",
    "\n",
    "- What is the median follower count for users in the following age groups: (`18-24`, `25-35`, `36-50`, `+50`)\n",
    "- The query should return a DataFrame that contains the following columns: (`age_group`, `median_follower_count`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356ade3a-8334-4cc0-900c-35aa6ac738e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "median_follower_per_age_group = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN Age BETWEEN 18 AND 24 THEN '18-24'\n",
    "            WHEN Age BETWEEN 25 AND 35 THEN '25-35'\n",
    "            WHEN Age BETWEEN 36 AND 50 THEN '36-50'\n",
    "            WHEN Age >= 51 then '50+'\n",
    "            ELSE 'NA'\n",
    "        End as age_group,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY follower_count) AS median_follower_count \n",
    "    FROM df_all\n",
    "    GROUP BY age_group\n",
    "    ORDER BY median_follower_count DESC\n",
    "\"\"\")\n",
    "display(median_follower_per_age_group)                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba944dc-262c-4306-ba86-cd14f469f7ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 6: Find how many users have joined each year?\n",
    "\n",
    "- Find how many users have joined between 2015 and 2020.\n",
    "- The query should return a DataFrame that contains the following columns: (`post_year`, `number_users_joined`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf890af-9989-455a-8994-05139e562d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_joined_per_year = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(YEAR FROM date_joined) AS post_year,\n",
    "        COUNT(DISTINCT(user_name)) AS number_users_joined\n",
    "    FROM df_all \n",
    "    WHERE EXTRACT(YEAR FROM date_joined) BETWEEN 2015 AND 2020\n",
    "    GROUP BY post_year\n",
    "\"\"\")\n",
    "display(users_joined_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32ffb1a-51af-412d-836b-d4d723842668",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 7: Find the median follower count of users based on their joining year\n",
    "\n",
    "- Find the median follower count of users have joined between 2015 and 2020.\n",
    "- Your query should return a DataFrame that contains the following columns: (`post_year`, `median_follower_count`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af432075-73a3-4f6e-b6f1-29e7319ae08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "median_follower_per_join_year = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(YEAR FROM date_joined) AS post_year,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY follower_count) AS median_follower_count\n",
    "    FROM df_all \n",
    "    WHERE EXTRACT(YEAR FROM date_joined) BETWEEN 2015 AND 2020\n",
    "    GROUP BY post_year\n",
    "\"\"\")\n",
    "display(median_follower_per_join_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0aeb38c-068e-41a9-b6c0-bf8626fc739b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Question 8: Find the median follower count of users based on their joining year and age group\n",
    "\n",
    "- Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of.\n",
    "- The query should return a DataFrame that contains the following columns: (`age_group`, `post_year`, `median_follower_count`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0aad072-5eb4-4f7a-b39e-9a298c30fddc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "median_follower_per_join_year_age_group = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CASE \n",
    "            WHEN Age BETWEEN 18 AND 24 THEN '18-24'\n",
    "            WHEN Age BETWEEN 25 AND 35 THEN '25-35'\n",
    "            WHEN Age BETWEEN 36 AND 50 THEN '36-50'\n",
    "            WHEN Age >= 51 then '50+'\n",
    "            ELSE 'NA'\n",
    "        End as age_group,\n",
    "        EXTRACT(YEAR FROM date_joined) AS post_year,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY follower_count) AS median_follower_count\n",
    "    FROM df_all \n",
    "    WHERE EXTRACT(YEAR FROM date_joined) BETWEEN 2015 AND 2020\n",
    "    GROUP BY age_group, post_year\n",
    "    Order by age_group, post_year\n",
    "\"\"\")\n",
    "display(median_follower_per_join_year_age_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a74c08-dac5-4aed-9bb1-ec878a6178c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Unmount S3 Bucket\n",
    "\n",
    "If required, the following cell will unmount the S3 bucket from Databricks Filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ebc75fa-299b-4222-b56f-1141b93cd378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_loader.unmount_s3_bucket()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "query_batch_data_mount",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
